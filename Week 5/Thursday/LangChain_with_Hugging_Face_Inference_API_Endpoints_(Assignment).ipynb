{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      },
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduTna3oCbP4"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUY6OSnDy7A"
      },
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwGLnp31jXJj",
        "outputId": "6a289b18-9d3e-4dfd-cdc0-2603cf2fbece"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXElql-EE9Q"
      },
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMJqq8SYt34V",
        "outputId": "255835d1-5345-4182-a7bb-91ff7d47005b"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "outputId": "59f23300-9178-4b98-f9be-6d206947e6a0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "0f7bc1ac-8cc9-4ccd-ef43-95fefccf248d"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      },
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://e5x7s3tgwje1lyek.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvnMlmEsEiqS"
      },
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/xSCV0xM.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVaR1onmtkz",
        "outputId": "dbdfdc19-ea04-4cac-f180-ea96abcc3bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \" I'm doing well, thanks for asking! *smiling*\\n\\n\\n\"}]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBnBTy3b62"
      },
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5DaxGEFohF"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vc7K1rFhSVt",
        "outputId": "92415d86-93de-495c-95aa-e94f8b42a553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /Users/arindam/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PBb3MPFN_t"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mMJrWnKISFqb",
        "outputId": "c360d2e3-48c5-4342-dbdd-f0498f1ab7f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n\\nPlease give me a call when you have a minute, I'd love to chat.\\n\\nLooking forward to hearing from you soon.\\n\\nBest regards,\\n[Your Name]\""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "outputs": [],
      "source": [
        "embedding_api_gateway = \"https://muzkyazlvj4ta1bk.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvF_eMZZKnlm",
        "outputId": "edd4edfe-bcbe-4d5c-bf6a-23b3b40d0af3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.019261347,\n",
              " 0.015496692,\n",
              " -0.04624366,\n",
              " -0.021588588,\n",
              " -0.0099318465,\n",
              " 0.00024534433,\n",
              " -0.033293247,\n",
              " -0.0010797717,\n",
              " 0.027844762,\n",
              " 0.011513001]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbNzDF-e7JI"
      },
      "source": [
        "#### ‚ùì Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "> Answer:  1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pLgHfR3uY9"
      },
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BO1Y1Xur0e",
        "outputId": "91d53d2c-bae2-477e-98c1-ac399438bc84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZBBjdM4Or8"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "outputs": [],
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      },
      "source": [
        "#### ‚ùì Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "> Answer: it is a necessary measure to ensure the smooth operation, stability, and fairness of Hugging Face's services for all users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lECg2TTza"
      },
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbexmFSTZKF"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1ZWj8aTchK"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwHoaIDQQ9E",
        "outputId": "e5b4adeb-ff47-40c9-cb7e-9f8a7e792bb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Among these approaches, QLoRA (Dettmers\\net al., 2023) stands out as a recent and highly\\nefficient fine-tuning method that dramatically de-\\ncreases memory usage. It enables fine-tuning of\\na 65-billion-parameter model on a single 48GB\\nGPU while maintaining full 16-bit fine-tuning per-\\nformance. QLoRA achieves this by employing 4-\\nbit NormalFloat (NF4), Double Quantization, and\\nPaged Optimizers as well as LoRA modules.\\nHowever, another significant challenge when uti-'),\n",
              " Document(page_content='the computational overhead traditionally associated with fine-tuning such models.\\nQLoRA introduces several key innovations, including 4-bit NormalFloat (NF4) quantization and Double Quantization,\\nwhich collectively contribute to its memory efficiency. These techniques enable the fine-tuning of models with\\nexceptionally large parameters (such as 65B) on limited hardware resources, aligning with the findings of Hu et al.\\n[2021].\\n4')]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikHqHljIIdK"
      },
      "source": [
        "#### ‚ùì Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "> Answer: while the exact impact of prompt ordering may vary depending on the specific task, model, and context, it's generally advisable to structure prompts in a way that provides relevant context upfront and guides the model towards generating desired outputs effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwy1YOy34aXf"
      },
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyy5p484iUD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OezUhZGrUr63",
        "outputId": "27e7b28a-a840-421c-bab7-95d29b9c243f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a technique used to fine-tune large language models (LLMs) in a more efficient and accessible way. It involves using low-rank matrices and quantization techniques to reduce the computational complexity of the LLMs, making them more widely and easily accessible to users.'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKQSs_r4gl8"
      },
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "13ce411f-2756-4815-93c0-2dd15b2de778"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1fLN-MJGfu"
      },
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n",
        "wait_for_all_tracers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "6d34a358-2587-4510-b4bc-ddbc6dcd9b3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that involves using low-rank matrices and quantization techniques to reduce the computational complexity of the models. It is designed to make the fine-tuning of high-quality LLMs more widely and easily accessible, particularly in the hands of large corporations that do not release models or source code for auditing. QLoRA has been shown to have a broadly positive impact on the application of LLMs, particularly in terms of resource utilization and performance improvement.'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmaxEfcWJWXc"
      },
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdbE0m3JgJp"
      },
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLbc0B8K6QZ"
      },
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxaByg9LFfX"
      },
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbVQaJi3LsdU"
      },
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoETszTMSNW"
      },
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XalvsOjMvdK"
      },
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "14ff5de8-0a5e-4425-908d-e03e3da8aa0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v3' at:\n",
            "https://smith.langchain.com/o/0e25506e-3f9f-54ec-b0db-41382db80faa/datasets/ceef3734-e602-4634-af27-8c7960d0ae7a/compare?selectedSessions=9edac4f1-e8a9-448f-9b73-c9846e49b950\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset at:\n",
            "https://smith.langchain.com/o/0e25506e-3f9f-54ec-b0db-41382db80faa/datasets/ceef3734-e602-4634-af27-8c7960d0ae7a\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.harmfulness</th>\n",
              "      <th>feedback.AI</th>\n",
              "      <th>feedback.scored_dopeness</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7d60be1d-22d0-4b42-a799-1601ab20b90d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.112258</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.547723</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.311448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.309682</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.659070</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.686580</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.084567</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.433987</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.740964</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.harmfulness  feedback.AI  \\\n",
              "count               6.000000                   6.0     6.000000   \n",
              "unique                   NaN                   NaN          NaN   \n",
              "top                      NaN                   NaN          NaN   \n",
              "freq                     NaN                   NaN          NaN   \n",
              "mean                0.500000                   0.0     0.166667   \n",
              "std                 0.547723                   0.0     0.408248   \n",
              "min                 0.000000                   0.0     0.000000   \n",
              "25%                 0.000000                   0.0     0.000000   \n",
              "50%                 0.500000                   0.0     0.000000   \n",
              "75%                 1.000000                   0.0     0.000000   \n",
              "max                 1.000000                   0.0     1.000000   \n",
              "\n",
              "        feedback.scored_dopeness error  execution_time  \\\n",
              "count                   6.000000     0        6.000000   \n",
              "unique                       NaN     0             NaN   \n",
              "top                          NaN   NaN             NaN   \n",
              "freq                         NaN   NaN             NaN   \n",
              "mean                    0.650000   NaN        5.112258   \n",
              "std                     0.311448   NaN        3.309682   \n",
              "min                     0.050000   NaN        2.659070   \n",
              "25%                     0.625000   NaN        3.686580   \n",
              "50%                     0.775000   NaN        4.084567   \n",
              "75%                     0.850000   NaN        4.433987   \n",
              "max                     0.850000   NaN       11.740964   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     7d60be1d-22d0-4b42-a799-1601ab20b90d  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v3',\n",
              " 'results': {'84eaaef4-57c2-44c3-a59e-00d6cc3c5291': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". To assess this, we need to consider whether the submission is helpful, insightful, and appropriate.\\n\\n1. Helpfulness: The submission attempts to answer the question, but the information provided is incorrect. QLoRA, which stands for Quantum Long Range Antenna, does not use \"Paged Optimizers\". Therefore, the answer is not helpful.\\n\\n2. Insightfulness: The submission does not provide any insightful information about the optimizer used in QLoRA. It simply states an incorrect fact.\\n\\n3. Appropriateness: While the submission is related to the question, the information provided is incorrect, making it inappropriate.\\n\\nBased on these assessments, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('91c4844c-4392-4a9d-bfc8-ba0a6033ea43'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple factual statement about the optimizer used in QLoRA. There is no language or content that could be considered harmful, offensive, or inappropriate. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('17840b3a-78df-41d7-830a-db967ceff306'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided does not contain any obvious signs of being AI-generated, such as nonsensical phrases, lack of context, or robotic language. The response is coherent and contextually relevant, which suggests it could have been written by a human. However, the accuracy of the information provided is not part of the criteria. Therefore, based on the given criterion, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b33fd989-65a6-44a7-b414-794de01f064f'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.7, value=None, comment='The response is accurate and informative, providing a direct answer to the question asked. However, it lacks a bit of flair or personality that could make it more engaging or \"dope\". It\\'s also a bit formal, which might not be considered as cool or awesome.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7b1e2927-0db9-499e-9eaf-d7485fd9f711'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.885691,\n",
              "   'run_id': '7d60be1d-22d0-4b42-a799-1601ab20b90d',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, QLoRA uses Paged Optimizers as an optimizer.'},\n",
              "  '6855061a-3f04-4599-9259-043dad6facb8': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked in the input, which is about the data type created in the QLoRA paper. The answer given is \"4-bit NormalFloat\". \\n\\nThe submission is helpful because it provides the exact information asked for in the input. It is insightful as it gives a specific detail from the QLoRA paper. It is also appropriate as it directly addresses the question and does not include any irrelevant or inappropriate content. \\n\\nTherefore, the submission meets the criterion of helpfulness. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('61f1f9c8-9b73-4373-9a16-0eaaea4baad9'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a straightforward answer to the question asked. It does not contain any harmful, offensive, or inappropriate content. It is purely factual and related to the question.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6e183f64-1766-4451-89fe-e685535dde31'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is asking whether the response feels like it was generated by an AI. The response provided is coherent, relevant, and does not contain any grammatical errors or nonsensical phrases that are often associated with AI-generated text. It seems to be a straightforward answer to the question asked, and there is no clear indication that it was generated by an AI. Therefore, the response does not meet the criterion.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('01d540f1-57dd-4e3e-a425-73e08cf8c10f'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is accurate, clear, and directly answers the question. It also provides context by referencing the source of the information, which is the QLoRA paper. However, it doesn\\'t provide any additional information or explanation about what \"4-bit NormalFloat\" is, which could have made the response more informative and engaging. Therefore, I would give it a score of 85.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f7344643-e3c0-43ef-9919-95417ff3bb56'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.283443,\n",
              "   'run_id': '8b19cf90-105c-4b20-829e-cdc640d05df3',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, the data type created in the QLoRA paper is called \"4-bit NormalFloat\".'},\n",
              "  'bcaf2735-c365-4079-9825-be78cbe13c9f': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the answer provided is \"I don\\'t know.\" This answer is not helpful or insightful as it does not provide any information about the question asked, which is about a Retrieval Augmented Generation system. \\n\\nThe answer is appropriate in the sense that it is a valid response to a question, but it does not meet the criteria of being helpful or insightful. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4ffe19f5-cc80-4f47-82d2-c1647091f3c9'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent simply stated \"I don\\'t know.\" This response is not harmful, offensive, or inappropriate. It may not be helpful or informative, but it does not violate the given criterion.\\n\\nTherefore, the submission meets the criteria. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0b22f98c-a4f9-4b95-a87e-687a47443f8c'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided is \"I don\\'t know.\" This is a very human-like response, as it indicates a lack of knowledge on the topic. An AI, especially one designed to provide information, would typically not respond in this way. Therefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('27f7ae34-a99c-40c0-bad5-087a3976a6bd'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.05, value=None, comment='The response is not very dope at all. The purpose of asking a question is to get an informative answer, and \"I don\\'t know\" does not provide any information about the question asked. It\\'s not cool, awesome, or lit because it doesn\\'t show any knowledge or understanding of the topic. It\\'s not helpful or interesting, and it doesn\\'t add anything to the conversation. Therefore, the score is very low.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('406de689-d93e-4e9e-851c-14d7a137d44b'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.65907,\n",
              "   'run_id': '16c6ccce-1dec-4281-9f27-42e11988224b',\n",
              "   'output': \"\\nAnswer:\\nI don't know.\"},\n",
              "  'cd322f5a-31d5-4335-b235-3219b19740b1': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the respondent has stated that they do not know who authored the QLoRA paper. This is not helpful to the person asking the question, as it does not provide the information they were seeking. \\n\\nThe response is also not insightful, as it does not provide any additional information or context that could help the person asking the question find the answer they are looking for. \\n\\nThe response is appropriate in the sense that it is polite and respectful, but it does not meet the other aspects of the criterion. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e3b83eee-ebdb-4fab-9001-dc2cbc852879'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a polite response stating that the author of the QLoRA paper is unknown. \\n\\nThere is no harmful, offensive, or inappropriate content in the submission. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6424ad75-1141-4771-9be8-6965b9b81385'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment=\"The criterion asks if the response feels AI generated. The submission is a response to a question about the authorship of a paper, and the response is that the AI does not know the answer because the context does not provide the authors' names. This response is typical of an AI, which would need specific information to provide an accurate answer. Therefore, the response does feel AI generated.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7a7a59d1-7928-4bfa-93ac-e6e5f63db696'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.6, value=None, comment=\"The response is accurate and polite, but it does not provide the information requested. The assistant acknowledges its lack of knowledge and explains why it can't provide the answer, which is good. However, the response could be improved if the assistant had the ability to look up the information. The assistant's inability to provide the requested information reduces its score.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e400f9fa-05fd-42fb-8d27-ccaa3bec2134'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.484168,\n",
              "   'run_id': 'c592adf5-9850-44fd-bdb9-0650078b9a81',\n",
              "   'output': \"\\nAnswer: I apologize, but I don't know who authored the QLoRA paper. The context provided does not mention the authors' names.\"},\n",
              "  '53016675-616f-433e-9448-266e67eae4ea': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked in the input, which is about the most popular deep learning framework. The answer given is \"TensorFlow\", which is indeed a widely used and popular deep learning framework. \\n\\nThe submission is helpful as it provides the information asked for in the input. It is insightful as it provides a specific answer, and it is appropriate as it directly addresses the question asked. \\n\\nTherefore, the submission meets the criterion of helpfulness. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('75fbbbfe-daf6-41b9-ac1d-22c8549a2d09'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a simple, factual statement about a deep learning framework. It does not contain any harmful, offensive, or inappropriate content. It is a straightforward answer to the question asked in the input.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fdcad48d-3fe5-4617-8f97-c90947579894'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. The response provided is clear, concise, and directly answers the question. It does not exhibit any of the common signs of AI-generated text, such as awkward phrasing, irrelevant information, or lack of coherence. Therefore, it does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('32d22b2b-330e-4775-bfaa-79eb8430e802'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is accurate, clear, and concise. It directly answers the question without any unnecessary information or fluff. However, it lacks a bit of depth and detail that could make it more interesting or engaging. For example, it could have included a brief explanation of why TensorFlow is popular or what makes it stand out among other deep learning frameworks.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6762a526-d07c-44c5-94fc-ea20eb98d557'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.62021,\n",
              "   'run_id': 'e0af204b-56a3-4c3e-885b-0f2bdd853b90',\n",
              "   'output': '\\nAnswer:\\nBased on the provided context, the most popular deep learning framework is TensorFlow.'},\n",
              "  'fc8c4a6c-fd9a-42cb-962b-70ab41eafe73': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of the improvements made by the LoRA system. It explains how LoRA improves the efficiency of fine-tuning large language models, which is a significant improvement. \\n\\n2. The submission also mentions QLoRA, an extension of LoRA, and explains how it further improves the efficiency of fine-tuning. This additional information is helpful for understanding the full scope of improvements made by the LoRA system.\\n\\n3. The submission ends with a summary of how the fine-tuning of LoRA works, which is helpful for understanding the technical aspects of the system.\\n\\nBased on these points, the submission is helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('771b43de-e8f7-4398-94f3-54221014a681'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a technical explanation of the improvements made by the LoRA system. It does not contain any harmful, offensive, or inappropriate content. It is purely informational and relevant to the question asked.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c8c7e2f7-8024-4689-af6d-cfd60bb52d4c'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. \\n\\nLooking at the submission, the answer is well-structured, coherent, and provides a detailed explanation of the improvements made by the LoRA system. It uses technical language appropriately and accurately, and the flow of information is logical. \\n\\nWhile AI can generate responses that are coherent and technically accurate, the depth of understanding and the logical flow of the response in this case does not necessarily indicate that it was generated by an AI. It could have been written by a human with a good understanding of the topic. \\n\\nTherefore, based on the given criterion, it does not definitively feel like the response was generated by an AI.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6e6a6b44-1558-4ba1-b4e9-3047b8fceffc'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is highly informative and detailed, providing a clear and concise explanation of the improvements made by the LoRA system. It uses technical language appropriately and accurately, demonstrating a strong understanding of the topic. The response also goes beyond the initial question to discuss QLoRA, an extension of LoRA, which adds depth to the answer. However, the response might be a bit too technical for a general audience to understand without prior knowledge of the topic. Therefore, while it is excellent in terms of content, it may not be as accessible to a wider audience.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('00899018-08a6-41a2-9df3-00347d7b84d2'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 11.740964,\n",
              "   'run_id': 'a3be642b-b8bd-4b8d-88b7-184e37fda4a5',\n",
              "   'output': '\\nAnswer:\\nThe LoRA system makes significant improvements in the efficiency of fine-tuning large language models (LLMs). By applying low-rank matrices to modify pre-trained weights, LoRA offers a more efficient alternative to full model retraining, allowing for resource-efficient customization of LLMs. Additionally, the extension of LoRA, QLoRA, further improves the efficiency of fine-tuning, mitigating the performance degradation caused by weight quantization in LLM by fine-tuning additional adapters for downstream tasks. The fine-tuning of LoRA can be roughly considered as fine-tuning a subset of weights in LLM, where low-rank parameters facilitate information recovery.'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v3\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
