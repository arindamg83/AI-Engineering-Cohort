Table of Contents
Part I
Item 1. Business
Our Company
NVIDIA pioneered accelerated computing to help solve the most challenging computational problems. NVIDIA is now a full-stack computing infrastructure
company with data-center-scale offerings that are reshaping industry.
Our full-stack includes the foundational CUDA programming model that runs on all NVIDIA GPUs, as well as hundreds of domain-specific software libraries,
software development kits, or SDKs, and Application Programming Interfaces, or APIs. This deep and broad software stack accelerates the performance and
eases the deployment of NVIDIA accelerated computing for computationally intensive workloads such as artificial intelligence, or AI, model training and
inference, data analytics, scientific computing, and 3D graphics, with vertical-specific optimizations to address industries ranging from healthcare and telecom to
automotive and manufacturing.
Our data-center-scale offerings are comprised of compute and networking solutions that can scale to tens of thousands of GPU-accelerated servers
interconnected to function as a single giant computer; this type of data center architecture and scale is needed for the development and deployment of modern
AI applications.
The GPU was initially used to simulate human imagination, enabling the virtual worlds of video games and films. Today, it also simulates human intelligence,
enabling a deeper understanding of the physical world. Its parallel processing capabilities, supported by thousands of computing cores, are essential for deep
learning algorithms. This form of AI, in which software writes itself by learning from large amounts of data, can serve as the brain of computers, robots and self-
driving cars that can perceive and understand the world. GPU-powered AI solutions are being developed by thousands of enterprises to deliver services and
products that would have been immensely difficult or even impossible with traditional coding. Examples include generative AI, which can create new content
such as text, code, images, audio, video, and molecule structures, and recommendation systems, which can recommend highly relevant content such as
products, services, media or ads using deep neural networks trained on vast datasets that capture the user preferences.
NVIDIA has a platform strategy, bringing together hardware, systems, software, algorithms, libraries, and services to create unique value for the markets we
serve. While the computing requirements of these end markets are diverse, we address them with a unified underlying architecture leveraging our GPUs and
networking and software stacks. The programmable nature of our architecture allows us to support several multi-billion-dollar end markets with the same
underlying technology by using a variety of software stacks developed either internally or by third-party developers and partners. The large and growing number
of developers and installed base across our platforms strengthens our ecosystem and increases the value of our platform to our customers.
Innovation is at our core. We have invested over $45.3 billion in research and development since our inception, yielding inventions that are essential to modern
computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market and redefined computer graphics. With our introduction of the CUDA
programming model in 2006, we opened the parallel processing capabilities of our GPU to a broad range of compute-intensive applications, paving the way for
the emergence of modern AI. In 2012, the AlexNet neural network, trained on NVIDIA GPUs, won the ImageNet computer image recognition competition,
marking the “Big Bang” moment of AI. We introduced our first Tensor Core GPU in 2017, built from the ground-up for the new era of AI, and our first autonomous
driving system-on-chips, or SoC, in 2018. Our acquisition of Mellanox in 2020 expanded our innovation canvas to include networking and led to the introduction
of a new processor class – the data processing unit, or DPU. Over the past 5 years, we have built full software stacks that run on top of our GPUs and CUDA to